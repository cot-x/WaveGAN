{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#from comet_ml import Experiment\n",
    "#experiment = Experiment()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "7Ccpy4OkFMEM"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.autograd import Variable\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "from tqdm import tqdm\n",
    "from pickle import load, dump\n",
    "from glob import glob\n",
    "import os\n",
    "import librosa\n",
    "import soundfile\n",
    "import random\n",
    "import argparse\n",
    "import time\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MuLaw:\n",
    "    def mulaw(self, x, mu=255):\n",
    "        return np.sign(x) * np.log1p(mu * np.abs(x)) / np.log1p(mu)\n",
    "    \n",
    "    def quantize(self, y, mu=255, offset=1):\n",
    "        return ((y + offset) / 2 * mu).astype(np.int64)\n",
    "    \n",
    "    def encode(self, x, mu=255):\n",
    "        return self.quantize(self.mulaw(x, mu), mu)\n",
    "    \n",
    "    def inv_mulaw(self, y, mu=255):\n",
    "        return np.sign(y) * (1.0 / mu) * ((1.0 + mu)**np.abs(y) - 1.0)\n",
    "    \n",
    "    def inv_quantize(self, y, mu=255):\n",
    "        return 2 * y.astype(np.float32) / mu - 1\n",
    "    \n",
    "    def decode(self, y, mu=255):\n",
    "        return self.inv_mulaw(self.inv_quantize(y, mu), mu)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Mish(nn.Module):\n",
    "    @staticmethod\n",
    "    def mish(x):\n",
    "        return x * torch.tanh(F.softplus(x))\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return Mish.mish(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GatedDilatedCausalConv1d(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels, kernel_size, dilation=1, **kwargs):\n",
    "        super().__init__()\n",
    "        self.padding = (kernel_size - 1) * dilation\n",
    "        self.conv = nn.Conv1d(in_channels, out_channels * 2, kernel_size=kernel_size, padding=self.padding, dilation=dilation, **kwargs)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        y = self.conv(x)\n",
    "        if self.padding > 0:\n",
    "            y = y[:, :, :-self.padding]\n",
    "        \n",
    "        a, b = y.split(y.size(1) // 2, dim=1)\n",
    "        y = torch.tanh(a) + torch.sigmoid(b)\n",
    "        \n",
    "        return y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FReLU(nn.Module):\n",
    "    def __init__(self, n_channel, kernel=3, stride=1, padding=1):\n",
    "        super().__init__()\n",
    "        self.funnel_condition = nn.Conv1d(n_channel, n_channel, kernel_size=kernel, stride=stride, padding=padding, groups=n_channel)\n",
    "        self.bn = nn.BatchNorm1d(n_channel)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        tx = self.bn(self.funnel_condition(x))\n",
    "        out = torch.max(x, tx)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For Checkerboard-Artifacts. (change from ConvTranspose1d)\n",
    "class UpsampleConv(nn.Module):\n",
    "    def __init__(self, in_features, out_features, kernel_size=3, stride=1, padding=1, scale_factor=2):\n",
    "        super().__init__()\n",
    "        self.upsample = nn.Upsample(scale_factor=scale_factor, mode='nearest')\n",
    "        self.conv = nn.Conv1d(in_features, out_features, kernel_size=kernel_size, stride=stride, padding=padding)\n",
    "        self.conv.weight.data.fill_(1.0 / np.prod(kernel_size))\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.upsample(x)\n",
    "        x = self.conv(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For Checkerboard-Artifacts. (for Discriminator)\n",
    "class PhaseShuffle(nn.Module):\n",
    "    def __init__(self, n_shift=2):\n",
    "        super().__init__()\n",
    "        self.n_shift = n_shift\n",
    "\n",
    "    def forward(self, x):\n",
    "        if self.n_shift == 0:\n",
    "            return x\n",
    "        \n",
    "        shift = random.randint(-self.n_shift, self.n_shift)\n",
    "        x = x[:, :, shift:] if shift > 0 else x[:, :, :shift] if shift < 0 else x\n",
    "        x_shift = x[:, :, :shift] if shift > 0 else x[:, :, shift:] if shift < 0 else None\n",
    "        if x_shift != None:\n",
    "            x_shift = x_shift.fliplr()\n",
    "            x = torch.cat([x, x_shift], dim=2)\n",
    "        \n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Generator(nn.Module):\n",
    "    def __init__(self, input_nc=1, output_nc=1, conv_dim=16, n_repeat=8):\n",
    "        super().__init__()\n",
    "        \n",
    "        out_features = conv_dim * 2 ** n_repeat\n",
    "        model = [\n",
    "            nn.Conv1d(input_nc, out_features, kernel_size=3, stride=1, padding=1),\n",
    "            nn.BatchNorm1d(out_features),\n",
    "            FReLU(out_features)\n",
    "        ]\n",
    "        in_features = out_features\n",
    "        \n",
    "        # Upsampling\n",
    "        out_features = in_features // 2\n",
    "        for _ in range(n_repeat):\n",
    "            model += [\n",
    "                UpsampleConv(in_features, out_features, scale_factor=2),\n",
    "                #nn.ConvTranspose1d(in_features, out_features, kernel_size=4, stride=2, padding=1),\n",
    "                nn.BatchNorm1d(out_features),\n",
    "                FReLU(out_features)\n",
    "            ]\n",
    "            in_features = out_features\n",
    "            out_features = in_features // 2\n",
    "            \n",
    "        # Output layer\n",
    "        model += [\n",
    "            nn.Conv1d(in_features, output_nc, kernel_size=3, stride=1, padding=1),\n",
    "            nn.Tanh()\n",
    "        ]\n",
    "\n",
    "        self.model = nn.Sequential(*model)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.model(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Discriminator(nn.Module):\n",
    "    def __init__(self, input_nc=1, conv_dim=16, n_repeat=8):\n",
    "        super().__init__()\n",
    "\n",
    "        out_features = conv_dim\n",
    "        model = [\n",
    "            GatedDilatedCausalConv1d(input_nc, out_features, kernel_size=3),\n",
    "            nn.BatchNorm1d(out_features)#,\n",
    "            #PhaseShuffle()\n",
    "        ]\n",
    "        in_features = out_features\n",
    "        \n",
    "        # downsampling\n",
    "        out_features = in_features * 2\n",
    "        for i in range(n_repeat):\n",
    "            model += [\n",
    "                nn.Conv1d(in_features, out_features, kernel_size=4, stride=2, padding=1),\n",
    "                FReLU(out_features),\n",
    "                nn.BatchNorm1d(out_features)#,\n",
    "                #PhaseShuffle()\n",
    "            ]\n",
    "            in_features = out_features\n",
    "            out_features = in_features * 2\n",
    "        \n",
    "        self.model = nn.Sequential(*model)\n",
    "\n",
    "        # PatchGAN\n",
    "        self.conv_patch = nn.Conv1d(in_features, 1, kernel_size=3, stride=1, padding=1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x =  self.model(x)\n",
    "        out = self.conv_patch(x)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class WaveDataset(Dataset):\n",
    "    def __init__(self, dir_path, ext='*', sound_length=65536, sampling_rate=16000, keep_alive=1000):\n",
    "        self.files = glob(os.path.join(dir_path, ext))\n",
    "        self.sound_length = sound_length\n",
    "        self.sampling_rate = sampling_rate\n",
    "        self.keep_alive = keep_alive\n",
    "        if(len(self.files) <= keep_alive):\n",
    "            self.contents = []\n",
    "            for file in self.files:\n",
    "                sound, _ = librosa.load(file, sr=self.sampling_rate)\n",
    "                self.contents += [sound]\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.files)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        if len(self.files) <= self.keep_alive:\n",
    "            sound = self.contents[index % len(self.files)]\n",
    "        else:\n",
    "            file = self.files[index % len(self.files)]\n",
    "            sound, _ = librosa.load(file, sr=self.sampling_rate)\n",
    "        \n",
    "        # Normalize.\n",
    "        max_amplitude = np.max(np.abs(sound))\n",
    "        if max_amplitude > 1:\n",
    "            sound /= max_amplitude\n",
    "\n",
    "        # Padding.\n",
    "        if sound.shape[0] < self.sound_length:\n",
    "            padding_length = self.sound_length - sound.shape[0]\n",
    "            left = np.zeros(padding_length//2)\n",
    "            right = np.zeros(padding_length - padding_length//2)\n",
    "            sound = np.concatenate([left, sound, right], axis=0)\n",
    "\n",
    "        # Floor.\n",
    "        if sound.shape[0] > self.sound_length:\n",
    "            start_index = random.randrange(sound.shape[0] - self.sound_length)\n",
    "            end_index = start_index + self.sound_length\n",
    "            sound = sound[start_index : end_index]\n",
    "        \n",
    "        sound = torch.from_numpy(sound.astype(np.float32))\n",
    "        \n",
    "        if sound.dim() < 2:\n",
    "            sound = sound.unsqueeze(0)\n",
    "            \n",
    "        return sound\n",
    "    \n",
    "    def save_wavfile(self, sounddata, filename):\n",
    "        soundfile.write(filename, sounddata.T, self.sampling_rate, format=\"WAV\", subtype=\"FLOAT\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Util:\n",
    "    @staticmethod\n",
    "    def loadWaveData(batch_size, dir_path, sound_length):\n",
    "        dataset = WaveDataset(dir_path, sound_length=sound_length)\n",
    "        return dataset, DataLoader(dataset, batch_size=batch_size, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Solver:\n",
    "    def __init__(self, args):\n",
    "        use_cuda = torch.cuda.is_available() if not args.cpu else False\n",
    "        self.device = torch.device(\"cuda\" if use_cuda else \"cpu\")\n",
    "        torch.backends.cudnn.benchmark = True\n",
    "        print(f'Use Device: {self.device}')\n",
    "        \n",
    "        self.args = args\n",
    "        \n",
    "        n_repeat = int(math.log(self.args.sound_length /  self.args.feed_dim, 2))\n",
    "        self.netG = Generator(n_repeat=n_repeat).to(self.device)\n",
    "        self.netD = Discriminator(n_repeat=n_repeat).to(self.device)\n",
    "\n",
    "        #self.netG.apply(self.weights_init)\n",
    "        self.netD.apply(self.weights_init)\n",
    "\n",
    "        self.optimizer_G = optim.Adam(self.netG.parameters(), lr=self.args.lr, betas=(0.5, 0.9))\n",
    "        self.optimizer_D = optim.Adam(self.netD.parameters(), lr=self.args.lr * self.args.mul_lr_dis, betas=(0.5, 0.9))\n",
    "        \n",
    "        self.load_dataset()\n",
    "        \n",
    "        self.pseudo_aug = 0.0\n",
    "        self.epoch = 0\n",
    "    \n",
    "    def weights_init(self, module):\n",
    "        if type(module) == nn.Linear or type(module) == nn.Conv1d or type(module) == nn.ConvTranspose1d:\n",
    "            nn.init.kaiming_normal_(module.weight)\n",
    "            if module.bias != None:\n",
    "                module.bias.data.fill_(0)\n",
    "            \n",
    "    def load_dataset(self):\n",
    "        self.dataset, self.dataloader = Util.loadWaveData(self.args.batch_size,\n",
    "                                                          self.args.sound_dir, self.args.sound_length)\n",
    "        self.max_iters = len(iter(self.dataloader))\n",
    "            \n",
    "    def save_state(self, epoch):\n",
    "        self.netG.cpu(), self.netD.cpu()\n",
    "        torch.save(self.netG.state_dict(), os.path.join(self.args.weight_dir, f'weight_G.{epoch}.pth'))\n",
    "        torch.save(self.netD.state_dict(), os.path.join(self.args.weight_dir, f'weight_D.{epoch}.pth'))\n",
    "        self.netG.to(self.device), self.netD.to(self.device)\n",
    "        \n",
    "    def load_state(self):\n",
    "        if (os.path.exists('weight_G.pth') and os.path.exists('weight_D.pth')):\n",
    "            self.netG.load_state_dict(torch.load('weight_G.pth', map_location=self.device))\n",
    "            self.netD.load_state_dict(torch.load('weight_D.pth', map_location=self.device))\n",
    "            print('Loaded network state.')\n",
    "    \n",
    "    def save_resume(self):\n",
    "        with open(os.path.join('.', f'resume.pkl'), 'wb') as f:\n",
    "            dump(self, f)\n",
    "    \n",
    "    def load_resume(self):\n",
    "        if os.path.exists('resume.pkl'):\n",
    "            with open(os.path.join('.', 'resume.pkl'), 'rb') as f:\n",
    "                print('Load resume.')\n",
    "                return load(f)\n",
    "        else:\n",
    "            return self\n",
    "        \n",
    "    def trainGAN(self, epoch, iters, max_iters, real_wav, a=0, b=1, c=1):\n",
    "        ### Train with LSGAN.\n",
    "        ### for example, (a, b, c) = 0, 1, 1 or (a, b, c) = -1, 1, 0\n",
    "        \n",
    "        feeds = torch.randn(real_wav.size(0), 1, self.args.feed_dim).to(self.device)\n",
    "        \n",
    "        # ================================================================================ #\n",
    "        #                             Train the discriminator                              #\n",
    "        # ================================================================================ #\n",
    "        \n",
    "        # Compute loss with real images.\n",
    "        real_src_score = self.netD(real_wav)\n",
    "        real_src_loss = torch.sum((real_src_score - b) ** 2)\n",
    "        \n",
    "        # Compute loss with fake images.\n",
    "        fake_wav = self.netG(feeds)\n",
    "        fake_src_score = self.netD(fake_wav)\n",
    "        \n",
    "        # Compute loss with fake images.\n",
    "        p = random.uniform(0, 1)\n",
    "        if 1 - self.pseudo_aug < p:\n",
    "            fake_src_loss = torch.sum((fake_src_score - b) ** 2) # Pseudo: fake is real.\n",
    "        else:\n",
    "            fake_src_loss = torch.sum((fake_src_score - a) ** 2)\n",
    "        \n",
    "        # Update Pseudo Augmentation.\n",
    "        lz = (torch.sign(torch.logit(real_src_score)).mean()\n",
    "              - torch.sign(torch.logit(fake_src_score)).mean()) / 2\n",
    "        if lz > 0.6:\n",
    "            self.pseudo_aug += 0.01\n",
    "        else:\n",
    "            self.pseudo_aug -= 0.01\n",
    "        self.pseudo_aug = min(1, max(0, self.pseudo_aug))\n",
    "        \n",
    "        # Backward and optimize.\n",
    "        d_loss = 0.5 * (real_src_loss + fake_src_loss) / self.args.batch_size\n",
    "        self.optimizer_D.zero_grad()\n",
    "        d_loss.backward()\n",
    "        self.optimizer_D.step()\n",
    "        \n",
    "        # Logging.\n",
    "        loss = {}\n",
    "        loss['D/loss'] = d_loss.item()\n",
    "        loss['D/pseudo_aug'] = self.pseudo_aug\n",
    "        \n",
    "        # ================================================================================ #\n",
    "        #                               Train the generator                                #\n",
    "        # ================================================================================ #\n",
    "        # Compute loss with reconstruction loss\n",
    "        fake_wav = self.netG(feeds)\n",
    "        fake_src_score = self.netD(fake_wav)\n",
    "        fake_src_loss = torch.sum((fake_src_score - c) ** 2)\n",
    "        \n",
    "        # Backward and optimize.\n",
    "        g_loss = 0.5 * fake_src_loss / self.args.batch_size\n",
    "        self.optimizer_G.zero_grad()\n",
    "        g_loss.backward()\n",
    "        self.optimizer_G.step()\n",
    "        \n",
    "        # Logging.\n",
    "        loss['G/loss'] = g_loss.item()\n",
    "        \n",
    "        # Save\n",
    "        if iters == max_iters:\n",
    "            self.save_state(epoch)\n",
    "            wav_name = str(epoch) + '_' + str(iters) + '.wav'\n",
    "            wav_path = os.path.join(self.args.result_dir, wav_name)\n",
    "            self.dataset.save_wavfile(fake_wav[0].detach().cpu(), wav_path)\n",
    "        \n",
    "        return loss\n",
    "    \n",
    "    def train(self, resume=True):\n",
    "        self.netG.train()\n",
    "        self.netD.train()\n",
    "        \n",
    "        while self.args.num_train > self.epoch:\n",
    "            self.epoch += 1\n",
    "            epoch_loss_G = 0.0\n",
    "            epoch_loss_D = 0.0\n",
    "            \n",
    "            self.mean_path_length = 0\n",
    "            for iters, data in enumerate(tqdm(self.dataloader)):\n",
    "                iters += 1\n",
    "                \n",
    "                data = data.to(self.device)\n",
    "                \n",
    "                loss = self.trainGAN(self.epoch, iters, self.max_iters, data)\n",
    "                \n",
    "                epoch_loss_D += loss['D/loss']\n",
    "                epoch_loss_G += loss['G/loss']\n",
    "                #experiment.log_metrics(loss)\n",
    "            \n",
    "            epoch_loss = epoch_loss_G + epoch_loss_D\n",
    "            \n",
    "            print(f'Epoch[{self.epoch}]'\n",
    "                  + f' Loss[G({epoch_loss_G}) + D({epoch_loss_D}) = {epoch_loss}]')\n",
    "                    \n",
    "            if resume:\n",
    "                self.save_resume()\n",
    "    \n",
    "    def generate(self, num=100):\n",
    "        self.netG.eval()\n",
    "        \n",
    "        for _ in range(num):\n",
    "            feeds = torch.randn(1, 1, self.args.feed_dim).to(self.device)\n",
    "            fake_wav = self.netG(feeds)\n",
    "            wav_path = os.path.join(self.args.result_dir, f'generated_{time.time()}.wav')\n",
    "            self.dataset.save_wavfile(fake_wav[0].detach().cpu(), wav_path)\n",
    "        print('New wave-file was generated.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main(args):\n",
    "    hyper_params = {}\n",
    "    hyper_params['Sound Dir'] = args.sound_dir\n",
    "    hyper_params['Result Dir'] = args.result_dir\n",
    "    hyper_params['Weight Dir'] = args.weight_dir\n",
    "    hyper_params['Sound Length'] = args.sound_length\n",
    "    hyper_params[\"Feed's dim\"] = args.feed_dim\n",
    "    hyper_params['Learning Rate'] = args.lr\n",
    "    hyper_params[\"Mul Discriminator's LR\"] = args.mul_lr_dis\n",
    "    hyper_params['Batch Size'] = args.batch_size\n",
    "    hyper_params['Num Train'] = args.num_train\n",
    "    \n",
    "    solver = Solver(args)\n",
    "    solver.load_state()\n",
    "    \n",
    "    if not args.noresume:\n",
    "        solver = solver.load_resume()\n",
    "    \n",
    "    if args.generate > 0:\n",
    "        solver.generate(args.generate)\n",
    "        return\n",
    "        \n",
    "    for key in hyper_params.keys():\n",
    "        print(f'{key}: {hyper_params[key]}')\n",
    "    #experiment.log_parameters(hyper_params)\n",
    "    \n",
    "    solver.train(not args.noresume)\n",
    "    \n",
    "    #experiment.end()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == '__main__':\n",
    "    parser = argparse.ArgumentParser()\n",
    "    parser.add_argument('--sound_dir', type=str, default='')\n",
    "    parser.add_argument('--result_dir', type=str, default='results')\n",
    "    parser.add_argument('--weight_dir', type=str, default='weights')\n",
    "    parser.add_argument('--sound_length', type=int, default=16384)\n",
    "    parser.add_argument('--feed_dim', type=int, default=64)\n",
    "    parser.add_argument('--lr', type=float, default=0.0001)\n",
    "    parser.add_argument('--mul_lr_dis', type=float, default=4)\n",
    "    parser.add_argument('--batch_size', type=int, default=8)\n",
    "    parser.add_argument('--num_train', type=int, default=100)\n",
    "    parser.add_argument('--cpu', action='store_true')\n",
    "    parser.add_argument('--generate', type=int, default=0)\n",
    "    parser.add_argument('--noresume', action='store_true')\n",
    "    \n",
    "    args, unknown = parser.parse_known_args()\n",
    "    \n",
    "    if not os.path.exists(args.result_dir):\n",
    "        os.mkdir(args.result_dir)\n",
    "    if not os.path.exists(args.weight_dir):\n",
    "        os.mkdir(args.weight_dir)\n",
    "    \n",
    "    main(args)"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "CycleGAN.ipynb",
   "private_outputs": true,
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
